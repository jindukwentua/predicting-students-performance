{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing python libraries\n",
    "#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set(style = 'darkgrid')\n",
    "import requests\n",
    "from io import StringIO\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold, cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import export_graphviz \n",
    "from IPython.display import Image\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, recall_score, roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_data = pd.read_csv('../data/cleaned_students_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode final_grade\n",
    "le = preprocessing.LabelEncoder()\n",
    "student_data.final_grade = le.fit_transform(student_data.final_grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the target columns from the predictive features\n",
    "X = student_data.drop(labels=['final_grade','final_score'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the target variable\n",
    "y = student_data.final_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummy varibles for the predictive features\n",
    "X = pd.get_dummies(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.3, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and configure the model\n",
    "# l1 regularization gives better results\n",
    "model = LogisticRegression()\n",
    "# fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "log_preds = model.predict(X_test)\n",
    "print('Accuracy : ',accuracy_score(y_test, log_preds))\n",
    "\n",
    "print(f'The accuracy score is: {accuracy_score(y_test, log_preds)} and the f1 score is {f1_score(y_test, log_preds)}')\n",
    "print(f'The recall score is: {recall_score(y_test, log_preds)}')\n",
    "print('\\n')\n",
    "print(f'{classification_report(y_test, log_preds)}')\n",
    "matrix = confusion_matrix(y_test, log_preds)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(matrix,annot = True, fmt = \"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define and configure the model\n",
    "model = DecisionTreeClassifier()\n",
    "# fit the model\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "dc_preds = model.predict(X_test)\n",
    "print('Accuracy : ',accuracy_score(y_test, dc_preds))\n",
    "\n",
    "print(f'The accuracy score is: {accuracy_score(y_test, dc_preds)} and the f1 score is {f1_score(y_test, dc_preds)}')\n",
    "print(f'The recall score is: {recall_score(y_test, dc_preds)}')\n",
    "print('\\n')\n",
    "print(f'{classification_report(y_test, dc_preds)}')\n",
    "matrix = confusion_matrix(y_test, dc_preds)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(matrix,annot = True, fmt = \"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and configure the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "rf_preds = model.predict(X_test)\n",
    "print('Accuracy : ',accuracy_score(y_test, rf_preds))\n",
    "\n",
    "print(f'The accuracy score is: {accuracy_score(y_test, rf_preds)} and the f1 score is {f1_score(y_test, rf_preds)}')\n",
    "print(f'The recall score is: {recall_score(y_test, rf_preds)}')\n",
    "print('\\n')\n",
    "print(f'{classification_report(y_test, rf_preds)}')\n",
    "matrix = confusion_matrix(y_test, rf_preds)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(matrix,annot = True, fmt = \"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the most important features that contribute most in predicting the target\n",
    "# Creating a dataframe of features and their respective importances\n",
    "#\n",
    "rf_impo_df = pd.DataFrame({'feature': X.columns, 'importance': np.round(model.feature_importances_, 4)}).set_index('feature').sort_values(by = 'importance', ascending = False)\n",
    "rf_impo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a bar chart of feature importance in descending order\n",
    "#\n",
    "rf_impo_df = rf_impo_df[:15].sort_values(by = 'importance', ascending = True)\n",
    "rf_impo_df.plot(kind = 'barh', figsize = (10, 10), color = 'purple')\n",
    "plt.legend(loc = 'center right')\n",
    "plt.title('Bar chart showing feature importance', color = 'indigo', fontsize = 14)\n",
    "plt.xlabel('Features', fontsize = 12, color = 'indigo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the most important features to build a random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_impo_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remodelling with the most important features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only important features and the y variable\n",
    "#\n",
    "X = X[rf_impo_df.index]\n",
    "\n",
    "y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "#\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n",
    "\n",
    "# define and configure the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "rf_preds = model.predict(X_test)\n",
    "print('Accuracy : ',accuracy_score(y_test, rf_preds))\n",
    "\n",
    "print(f'The accuracy score is: {accuracy_score(y_test, rf_preds)} and the f1 score is {f1_score(y_test, rf_preds)}')\n",
    "print(f'The recall score is: {recall_score(y_test, rf_preds)}')\n",
    "print('\\n')\n",
    "print(f'{classification_report(y_test, rf_preds)}')\n",
    "matrix = confusion_matrix(y_test, rf_preds)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(matrix,annot = True, fmt = \"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previewing the parameters to tune\n",
    "#\n",
    "RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of parameters to tune\n",
    "#\n",
    "params = {'n_estimators': [10, 20, 30, 50, 100],\n",
    "         'max_depth': [1, 2, 3, 4, 5]}\n",
    "\n",
    "# Setting the number of folds to 10 and instantiating the model\n",
    "# \n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "search = GridSearchCV(model, param_grid=params, scoring = 'f1', cv = kfold)\n",
    "\n",
    "# Fitting the grid search with the X and the y variables\n",
    "#\n",
    "search.fit(X, y)\n",
    "\n",
    "# Checking for the best parameters\n",
    "#\n",
    "print(f'The best parameters are: {search.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the best parameters to the model\n",
    "\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "#\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n",
    "\n",
    "## define and configure the model\n",
    "model = RandomForestClassifier(max_depth = 3, n_estimators = 100)\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "rf_preds = model.predict(X_test)\n",
    "print('Accuracy : ',accuracy_score(y_test, rf_preds))\n",
    "\n",
    "print(f'The accuracy score is: {accuracy_score(y_test, rf_preds)} and the f1 score is {f1_score(y_test, rf_preds)}')\n",
    "print(f'The recall score is: {recall_score(y_test, rf_preds)}')\n",
    "print('\\n')\n",
    "print(f'{classification_report(y_test, rf_preds)}')\n",
    "matrix = confusion_matrix(y_test, rf_preds)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(matrix,annot = True, fmt = \"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parameter tuning has increased the f1 score*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation to check for the stability of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cross validation of ten folds\n",
    "#\n",
    "scores = cross_val_score(model, X, y, scoring = 'f1', cv = 10)\n",
    "\n",
    "# Calculating the mean of the cross validation scores\n",
    "#\n",
    "print(f'Mean of cross validation scores is {scores.mean()}')\n",
    "\n",
    "# Calculating the variance of the cross validation scores from the mean\n",
    "#\n",
    "print(f'Standard deviation of the cross validation scores is {scores.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting ROC Index Curve and comparing AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_fpr, logistic_tpr, threshold = roc_curve(y_test, log_preds)\n",
    "auc_logistic = auc(logistic_fpr, logistic_tpr)\n",
    "\n",
    "decision_tree_fpr, decision_tree_tpr, threshold = roc_curve(y_test, dc_preds)\n",
    "auc_decision_tree = auc(decision_tree_fpr, decision_tree_tpr)\n",
    "\n",
    "random_forest_fpr, random_forest_tpr, threshold = roc_curve(y_test, rf_preds)\n",
    "auc_random_forest = auc(random_forest_fpr, random_forest_tpr)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 5), dpi=100)\n",
    "plt.plot(logistic_fpr, logistic_tpr, marker='.', label='Logistic (auc = %0.3f)' % auc_logistic)\n",
    "plt.plot(decision_tree_fpr, decision_tree_tpr, marker='.', label='decision_tree (auc = %0.3f)' % auc_decision_tree)\n",
    "plt.plot(random_forest_fpr, random_forest_tpr, marker='.', label='random_forest (auc = %0.3f)' % auc_random_forest)\n",
    "\n",
    "plt.xlabel('False Positive Rate -->')\n",
    "plt.ylabel('True Positive Rate -->')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above chart, Random Forest offers the best RUC and AUC performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
